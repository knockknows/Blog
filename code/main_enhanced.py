from fastapi import FastAPI, HTTPException, status, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, HttpUrl, Field
from playwright.async_api import async_playwright, Browser, TimeoutError as PlaywrightTimeout
from typing import Literal, Optional, List
from contextlib import asynccontextmanager
from jose import JWTError, jwt
from passlib.context import CryptContext
from datetime import datetime, timedelta
import asyncio
import os
import logging
import asyncpg

# ==========================================
# ë¡œê¹… ì„¤ì •
# ==========================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==========================================
# JWT ì„¤ì •
# ==========================================
SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key-change-this-in-production-2024")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

# ë¹„ë°€ë²ˆí˜¸ í•´ì‹± ì„¤ì •
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# HTTPBearer ìŠ¤í‚´ (JWT í† í° ê²€ì¦ìš©)
security = HTTPBearer()

# ==========================================
# ì „ì—­ ë³€ìˆ˜
# ==========================================
browser: Optional[Browser] = None
db_pool: Optional[asyncpg.Pool] = None

# ê°„ë‹¨í•œ ì‚¬ìš©ì ë°ì´í„°ë² ì´ìŠ¤ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” DB ì‚¬ìš©)
# ë¹„ë°€ë²ˆí˜¸: "secure_password_123"ì˜ bcrypt í•´ì‹œ
FAKE_USERS_DB = {
    "n8n_user": {
        "username": "n8n_user",
        "hashed_password": "$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewY5GyYzpLaOALem"  # secure_password_123
    }
}

# ==========================================
# PostgreSQL ì„¤ì •
# ==========================================
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://postgres:postgres@localhost:5432/scraper_db"
)

# ==========================================
# Lifespan Context Manager
# ==========================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì „ì²´ ìƒëª…ì£¼ê¸°ë¥¼ ê´€ë¦¬í•´ìš”."""
    global browser, db_pool
    playwright_url = os.getenv("PLAYWRIGHT_SERVER_URL", "ws://playwright:3000")
    playwright_instance = None
    
    logger.info("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")
    
    try:
        # 1. Playwright ë¸Œë¼ìš°ì € ì—°ê²°
        logger.info(f"ğŸ“¡ Playwright Serverì— ì—°ê²° ì‹œë„: {playwright_url}")
        playwright_instance = await async_playwright().start()
        browser = await playwright_instance.chromium.connect(
            playwright_url,
            timeout=10000
        )
        logger.info("âœ… Playwright ë¸Œë¼ìš°ì € ì—°ê²° ì™„ë£Œ!")
        
        # 2. PostgreSQL ì—°ê²° í’€ ìƒì„±
        logger.info(f"ğŸ—„ï¸  PostgreSQLì— ì—°ê²° ì‹œë„: {DATABASE_URL.split('@')[1] if '@' in DATABASE_URL else DATABASE_URL}")
        db_pool = await asyncpg.create_pool(
            DATABASE_URL,
            min_size=2,
            max_size=10,
            command_timeout=60
        )
        logger.info("âœ… PostgreSQL ì—°ê²° í’€ ìƒì„± ì™„ë£Œ!")
        
        # 3. í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´)
        async with db_pool.acquire() as conn:
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS processed_urls (
                    id SERIAL PRIMARY KEY,
                    url TEXT UNIQUE NOT NULL,
                    title TEXT,
                    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    success BOOLEAN DEFAULT TRUE
                )
            ''')
            logger.info("âœ… PostgreSQL í…Œì´ë¸” í™•ì¸/ìƒì„± ì™„ë£Œ!")
        
        logger.info("ğŸ’¡ ëª¨ë“  ì´ˆê¸°í™” ì™„ë£Œ! ìš”ì²­ì„ ë°›ì„ ì¤€ë¹„ê°€ ë˜ì—ˆì–´ìš”!")
        
        yield
        
    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise
    
    finally:
        logger.info("ğŸ›‘ FastAPI ì„œë²„ ì¢…ë£Œ ì¤‘...")
        
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        if browser:
            await browser.close()
            logger.info("âœ… Playwright ë¸Œë¼ìš°ì € ì—°ê²° ì¢…ë£Œ ì™„ë£Œ")
        
        if playwright_instance:
            await playwright_instance.stop()
            logger.info("âœ… Playwright ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ì™„ë£Œ")
        
        # PostgreSQL ì—°ê²° í’€ ì¢…ë£Œ
        if db_pool:
            await db_pool.close()
            logger.info("âœ… PostgreSQL ì—°ê²° í’€ ì¢…ë£Œ ì™„ë£Œ")

# ==========================================
# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
# ==========================================
app = FastAPI(
    title="Playwright Scraper API with JWT & PostgreSQL",
    description="JWT ì¸ì¦, ë³‘ë ¬ ì²˜ë¦¬, PostgreSQL ì¤‘ë³µ ì œê±° ê¸°ëŠ¥ì„ ê°–ì¶˜ ìŠ¤í¬ë˜í¼ API",
    version="3.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# Pydantic ëª¨ë¸ ì •ì˜
# ==========================================

class LoginRequest(BaseModel):
    """ë¡œê·¸ì¸ ìš”ì²­ ëª¨ë¸"""
    username: str
    password: str

class TokenResponse(BaseModel):
    """JWT í† í° ì‘ë‹µ ëª¨ë¸"""
    access_token: str
    token_type: str = "bearer"
    expires_in: int

class ScrapeRequest(BaseModel):
    """ë‹¨ì¼ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ëª¨ë¸"""
    url: HttpUrl
    wait_for: Literal["load", "domcontentloaded", "networkidle", "commit"] = "networkidle"
    timeout: int = Field(default=30000, ge=1000, le=120000)
    screenshot: bool = Field(default=False)
    block_resources: bool = Field(default=False)

class BatchScrapeRequest(BaseModel):
    """ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ìš”ì²­ ëª¨ë¸"""
    urls: List[ScrapeRequest]
    max_concurrent: int = Field(
        default=5,
        ge=1,
        le=10,
        description="ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜ (1~10)"
    )
    check_duplicates: bool = Field(
        default=True,
        description="PostgreSQLì—ì„œ ì¤‘ë³µ URL ì²´í¬ ì—¬ë¶€"
    )

class ScrapeResponse(BaseModel):
    """ë‹¨ì¼ ìŠ¤í¬ë˜í•‘ ì‘ë‹µ ëª¨ë¸"""
    url: str
    html: str
    title: str
    success: bool
    is_duplicate: bool = False
    screenshot_base64: Optional[str] = None
    scraped_at: str
    response_time_ms: int
    error: Optional[str] = None

class BatchScrapeResponse(BaseModel):
    """ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì‘ë‹µ ëª¨ë¸"""
    total_urls: int
    successful: int
    failed: int
    skipped_duplicates: int
    results: List[ScrapeResponse]
    total_time_ms: int

# ==========================================
# JWT ê´€ë ¨ í•¨ìˆ˜
# ==========================================

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """ë¹„ë°€ë²ˆí˜¸ í•´ì‹±"""
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    """JWT ì•¡ì„¸ìŠ¤ í† í° ìƒì„±"""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)) -> str:
    """JWT í† í° ê²€ì¦ (ì˜ì¡´ì„± ì£¼ì…ìš©)"""
    token = credentials.credentials
    
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        
        if username is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ì¦ í† í°ì´ì—ìš”",
                headers={"WWW-Authenticate": "Bearer"},
            )
        
        return username
    
    except JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="í† í° ê²€ì¦ì— ì‹¤íŒ¨í–ˆì–´ìš”",
            headers={"WWW-Authenticate": "Bearer"},
        )

# ==========================================
# PostgreSQL ì¤‘ë³µ ì²´í¬ í•¨ìˆ˜
# ==========================================

async def check_url_processed(url: str) -> bool:
    """URLì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    global db_pool
    
    if db_pool is None:
        logger.warning("PostgreSQL ì—°ê²° í’€ì´ ì—†ì–´ìš”. ì¤‘ë³µ ì²´í¬ë¥¼ ê±´ë„ˆë›°ì–´ìš”.")
        return False
    
    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchval(
                'SELECT EXISTS(SELECT 1 FROM processed_urls WHERE url = $1)',
                url
            )
            return result
    except Exception as e:
        logger.error(f"ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {str(e)}")
        return False

async def save_processed_url(url: str, title: str, success: bool = True):
    """ì²˜ë¦¬ëœ URLì„ PostgreSQLì— ì €ì¥"""
    global db_pool
    
    if db_pool is None:
        logger.warning("PostgreSQL ì—°ê²° í’€ì´ ì—†ì–´ìš”. URL ì €ì¥ì„ ê±´ë„ˆë›°ì–´ìš”.")
        return
    
    try:
        async with db_pool.acquire() as conn:
            await conn.execute(
                '''
                INSERT INTO processed_urls (url, title, success, processed_at)
                VALUES ($1, $2, $3, CURRENT_TIMESTAMP)
                ON CONFLICT (url) DO UPDATE
                SET title = EXCLUDED.title,
                    success = EXCLUDED.success,
                    processed_at = CURRENT_TIMESTAMP
                ''',
                url, title, success
            )
            logger.debug(f"URL ì €ì¥ ì™„ë£Œ: {url}")
    except Exception as e:
        logger.error(f"URL ì €ì¥ ì˜¤ë¥˜: {str(e)}")

# ==========================================
# ë‹¨ì¼ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ (ë‚´ë¶€ìš©)
# ==========================================

async def scrape_single_url(
    request: ScrapeRequest,
    semaphore: Optional[asyncio.Semaphore] = None,
    check_duplicate: bool = False
) -> ScrapeResponse:
    """ë‹¨ì¼ URL ìŠ¤í¬ë˜í•‘ (ë³‘ë ¬ ì²˜ë¦¬ìš© ë‚´ë¶€ í•¨ìˆ˜)"""
    global browser
    
    start_time = datetime.now()
    page = None
    
    # ì„¸ë§ˆí¬ì–´ê°€ ìˆìœ¼ë©´ ì‚¬ìš© (ë™ì‹œ ì‹¤í–‰ ì œí•œ)
    if semaphore:
        await semaphore.acquire()
    
    try:
        # ì¤‘ë³µ ì²´í¬
        if check_duplicate:
            is_duplicate = await check_url_processed(str(request.url))
            if is_duplicate:
                logger.info(f"â­ï¸  ì¤‘ë³µ URL ê±´ë„ˆë›°ê¸°: {request.url}")
                response_time = int((datetime.now() - start_time).total_seconds() * 1000)
                return ScrapeResponse(
                    url=str(request.url),
                    html="",
                    title="",
                    success=True,
                    is_duplicate=True,
                    scraped_at=datetime.now().isoformat(),
                    response_time_ms=response_time
                )
        
        logger.info(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {request.url}")
        
        # í˜ì´ì§€ ìƒì„±
        page = await browser.new_page(viewport={"width": 1920, "height": 1080})
        
        # ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
        if request.block_resources:
            await page.route("**/*", lambda route: (
                route.abort() if route.request.resource_type in ["image", "font", "stylesheet"]
                else route.continue_()
            ))
        
        page.set_default_timeout(request.timeout)
        
        # í˜ì´ì§€ ì´ë™
        await page.goto(
            str(request.url),
            wait_until=request.wait_for,
            timeout=request.timeout
        )
        
        # ì»¨í…ì¸  ì¶”ì¶œ
        html_content = await page.content()
        page_title = await page.title()
        
        # ìŠ¤í¬ë¦°ìƒ·
        screenshot_base64 = None
        if request.screenshot:
            import base64
            screenshot_bytes = await page.screenshot(full_page=True, type="png")
            screenshot_base64 = base64.b64encode(screenshot_bytes).decode('utf-8')
        
        response_time = int((datetime.now() - start_time).total_seconds() * 1000)
        
        # PostgreSQLì— ì €ì¥
        if check_duplicate:
            await save_processed_url(str(request.url), page_title, True)
        
        logger.info(f"âœ“ í¬ë¡¤ë§ ì™„ë£Œ: {request.url} ({response_time}ms)")
        
        return ScrapeResponse(
            url=str(request.url),
            html=html_content,
            title=page_title,
            success=True,
            is_duplicate=False,
            screenshot_base64=screenshot_base64,
            scraped_at=datetime.now().isoformat(),
            response_time_ms=response_time
        )
    
    except PlaywrightTimeout:
        response_time = int((datetime.now() - start_time).total_seconds() * 1000)
        logger.error(f"â±ï¸  íƒ€ì„ì•„ì›ƒ: {request.url}")
        return ScrapeResponse(
            url=str(request.url),
            html="",
            title="",
            success=False,
            scraped_at=datetime.now().isoformat(),
            response_time_ms=response_time,
            error="í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ"
        )
    
    except Exception as e:
        response_time = int((datetime.now() - start_time).total_seconds() * 1000)
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {request.url} - {str(e)}")
        return ScrapeResponse(
            url=str(request.url),
            html="",
            title="",
            success=False,
            scraped_at=datetime.now().isoformat(),
            response_time_ms=response_time,
            error=str(e)
        )
    
    finally:
        if page:
            await page.close()
        
        if semaphore:
            semaphore.release()

# ==========================================
# ì—”ë“œí¬ì¸íŠ¸: ë¡œê·¸ì¸ (JWT ë°œê¸‰)
# ==========================================

@app.post("/login", response_model=TokenResponse, tags=["Authentication"])
async def login(login_data: LoginRequest):
    """
    ë¡œê·¸ì¸í•˜ì—¬ JWT ì•¡ì„¸ìŠ¤ í† í°ì„ ë°œê¸‰ë°›ì•„ìš”.
    
    - **username**: ì‚¬ìš©ì ì´ë¦„ (ì˜ˆ: n8n_user)
    - **password**: ë¹„ë°€ë²ˆí˜¸ (ì˜ˆ: secure_password_123)
    """
    user = FAKE_USERS_DB.get(login_data.username)
    
    if not user or not verify_password(login_data.password, user["hashed_password"]):
        logger.warning(f"ë¡œê·¸ì¸ ì‹¤íŒ¨ ì‹œë„: {login_data.username}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ì‚¬ìš©ì ì´ë¦„ ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ì–´ìš”",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # JWT í† í° ìƒì„±
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user["username"]},
        expires_delta=access_token_expires
    )
    
    logger.info(f"âœ… ë¡œê·¸ì¸ ì„±ê³µ: {login_data.username}")
    
    return TokenResponse(
        access_token=access_token,
        expires_in=ACCESS_TOKEN_EXPIRE_MINUTES * 60
    )

# ==========================================
# ì—”ë“œí¬ì¸íŠ¸: í—¬ìŠ¤ ì²´í¬
# ==========================================

@app.get("/", tags=["Health"])
async def root():
    """API ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "ok",
        "message": "Playwright Scraper API with JWT & PostgreSQL",
        "version": "3.0.0"
    }

@app.get("/health", tags=["Health"])
async def health_check():
    """ë¸Œë¼ìš°ì € ë° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸"""
    global browser, db_pool
    
    browser_status = "not connected"
    db_status = "not connected"
    
    # ë¸Œë¼ìš°ì € ì²´í¬
    if browser:
        try:
            test_page = await browser.new_page()
            await test_page.close()
            browser_status = "connected"
        except Exception as e:
            browser_status = f"error: {str(e)}"
    
    # PostgreSQL ì²´í¬
    if db_pool:
        try:
            async with db_pool.acquire() as conn:
                await conn.fetchval('SELECT 1')
            db_status = "connected"
        except Exception as e:
            db_status = f"error: {str(e)}"
    
    is_healthy = browser_status == "connected" and db_status == "connected"
    
    return {
        "status": "healthy" if is_healthy else "unhealthy",
        "browser": browser_status,
        "database": db_status,
        "optimization": "lifespan + connection pooling"
    }

# ==========================================
# ì—”ë“œí¬ì¸íŠ¸: ë‹¨ì¼ URL ìŠ¤í¬ë˜í•‘
# ==========================================

@app.post("/scrape", response_model=ScrapeResponse, tags=["Scraping"])
async def scrape_url(
    request: ScrapeRequest,
    username: str = Depends(verify_token)
):
    """
    ë‹¨ì¼ URLì„ ìŠ¤í¬ë˜í•‘í•´ìš”. (JWT ì¸ì¦ í•„ìš”)
    
    - **Authorization í—¤ë”**: Bearer {access_token}
    """
    global browser
    
    if browser is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë¸Œë¼ìš°ì €ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ì–´ìš”"
        )
    
    logger.info(f"ğŸ“ ì‚¬ìš©ì '{username}'ê°€ ë‹¨ì¼ ìŠ¤í¬ë˜í•‘ ìš”ì²­: {request.url}")
    
    result = await scrape_single_url(request, check_duplicate=False)
    
    if not result.success:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "success": False,
                "error": result.error,
                "url": result.url
            }
        )
    
    return result

# ==========================================
# ì—”ë“œí¬ì¸íŠ¸: ë³‘ë ¬ ìŠ¤í¬ë˜í•‘
# ==========================================

@app.post("/scrape/batch", response_model=BatchScrapeResponse, tags=["Scraping"])
async def scrape_batch(
    request: BatchScrapeRequest,
    username: str = Depends(verify_token)
):
    """
    ì—¬ëŸ¬ URLì„ ë³‘ë ¬ë¡œ ìŠ¤í¬ë˜í•‘í•´ìš”. (JWT ì¸ì¦ í•„ìš”)
    
    - **Authorization í—¤ë”**: Bearer {access_token}
    - **max_concurrent**: ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
    - **check_duplicates**: PostgreSQL ì¤‘ë³µ ì²´í¬ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    """
    global browser
    
    if browser is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë¸Œë¼ìš°ì €ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ì–´ìš”"
        )
    
    if not request.urls:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="URL ëª©ë¡ì´ ë¹„ì–´ìˆì–´ìš”"
        )
    
    logger.info(
        f"ğŸ“ ì‚¬ìš©ì '{username}'ê°€ ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ìš”ì²­: "
        f"{len(request.urls)}ê°œ URL, ë™ì‹œì²˜ë¦¬ {request.max_concurrent}ê°œ"
    )
    
    start_time = datetime.now()
    
    # ì„¸ë§ˆí¬ì–´ ìƒì„± (ë™ì‹œ ì‹¤í–‰ ì œí•œ)
    semaphore = asyncio.Semaphore(request.max_concurrent)
    
    # ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    tasks = [
        scrape_single_url(url_request, semaphore, request.check_duplicates)
        for url_request in request.urls
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=False)
    
    # í†µê³„ ê³„ì‚°
    successful = sum(1 for r in results if r.success and not r.is_duplicate)
    failed = sum(1 for r in results if not r.success)
    skipped_duplicates = sum(1 for r in results if r.is_duplicate)
    
    total_time = int((datetime.now() - start_time).total_seconds() * 1000)
    
    logger.info(
        f"âœ… ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {len(results)}ê°œ "
        f"(ì„±ê³µ: {successful}, ì‹¤íŒ¨: {failed}, ì¤‘ë³µ: {skipped_duplicates}, {total_time}ms)"
    )
    
    return BatchScrapeResponse(
        total_urls=len(request.urls),
        successful=successful,
        failed=failed,
        skipped_duplicates=skipped_duplicates,
        results=results,
        total_time_ms=total_time
    )

# ==========================================
# ì—”ë“œí¬ì¸íŠ¸: ì²˜ë¦¬ëœ URL ì¡°íšŒ
# ==========================================

@app.get("/processed-urls", tags=["Database"])
async def get_processed_urls(
    limit: int = 100,
    offset: int = 0,
    username: str = Depends(verify_token)
):
    """
    PostgreSQLì— ì €ì¥ëœ ì²˜ë¦¬ëœ URL ëª©ë¡ì„ ì¡°íšŒí•´ìš”. (JWT ì¸ì¦ í•„ìš”)
    
    - **limit**: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)
    - **offset**: ê±´ë„ˆë›¸ ê°œìˆ˜ (ê¸°ë³¸ê°’: 0)
    """
    global db_pool
    
    if db_pool is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ì–´ìš”"
        )
    
    try:
        async with db_pool.acquire() as conn:
            # ì´ ê°œìˆ˜ ì¡°íšŒ
            total_count = await conn.fetchval('SELECT COUNT(*) FROM processed_urls')
            
            # URL ëª©ë¡ ì¡°íšŒ
            rows = await conn.fetch(
                '''
                SELECT id, url, title, processed_at, success
                FROM processed_urls
                ORDER BY processed_at DESC
                LIMIT $1 OFFSET $2
                ''',
                limit, offset
            )
            
            urls = [
                {
                    "id": row["id"],
                    "url": row["url"],
                    "title": row["title"],
                    "processed_at": row["processed_at"].isoformat(),
                    "success": row["success"]
                }
                for row in rows
            ]
            
            return {
                "total_count": total_count,
                "limit": limit,
                "offset": offset,
                "urls": urls
            }
    
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ëœ URL ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}"
        )

# ==========================================
# ì—”ë“œí¬ì¸íŠ¸: ì²˜ë¦¬ëœ URL ì‚­ì œ
# ==========================================

@app.delete("/processed-urls/{url_id}", tags=["Database"])
async def delete_processed_url(
    url_id: int,
    username: str = Depends(verify_token)
):
    """
    PostgreSQLì—ì„œ íŠ¹ì • URLì„ ì‚­ì œí•´ìš”. (JWT ì¸ì¦ í•„ìš”)
    
    - **url_id**: ì‚­ì œí•  URLì˜ ID
    """
    global db_pool
    
    if db_pool is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ì–´ìš”"
        )
    
    try:
        async with db_pool.acquire() as conn:
            result = await conn.execute(
                'DELETE FROM processed_urls WHERE id = $1',
                url_id
            )
            
            if result == "DELETE 0":
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"ID {url_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”"
                )
            
            return {"message": f"ID {url_id} ì‚­ì œ ì™„ë£Œ"}
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"URL ì‚­ì œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì‚­ì œ ì˜¤ë¥˜: {str(e)}"
        )

# ==========================================
# ì—”ë“œí¬ì¸íŠ¸: ì„œë²„ ì •ë³´
# ==========================================

@app.get("/info", tags=["Info"])
async def server_info():
    """í˜„ì¬ ì„œë²„ì˜ ìµœì í™” ìƒíƒœì™€ ì—”ë“œí¬ì¸íŠ¸ ì •ë³´"""
    global browser, db_pool
    
    return {
        "version": "3.0.0",
        "features": {
            "jwt_authentication": "enabled",
            "parallel_scraping": "enabled",
            "duplicate_check": "postgresql"
        },
        "optimization": {
            "lifespan": "enabled",
            "browser_reuse": "enabled",
            "connection_pooling": "enabled"
        },
        "status": {
            "browser_connected": browser is not None,
            "database_connected": db_pool is not None
        },
        "endpoints": {
            "authentication": "/login",
            "single_scrape": "/scrape",
            "batch_scrape": "/scrape/batch",
            "processed_urls": "/processed-urls",
            "health": "/health"
        }
    }
